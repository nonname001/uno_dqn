{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KOMkO0tzYvn"
      },
      "source": [
        "# **Installing Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnZt1Ym-zlgg",
        "outputId": "e71582db-7e2e-424a-e6eb-c53f45785e18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r0% [1 InRelease gpgv 1,581 B] [Connecting to archive.ubuntu.com] [Connecting to\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r0% [1 InRelease gpgv 1,581 B] [Connecting to archive.ubuntu.com] [Connecting to\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [Waiting for headers] [W\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (185.125.190.39\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Fetched 252 kB in 1s (195 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "freeglut3-dev is already the newest version (2.8.1-3).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.10).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 56 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imageio==2.4.0 in /usr/local/lib/python3.7/dist-packages (2.4.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio==2.4.0) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio==2.4.0) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tf-agents[reverb] in /usr/local/lib/python3.7/dist-packages (0.12.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.21.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.15.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.14.1)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (2.1.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (7.1.2)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (4.2.0)\n",
            "Requirement already satisfied: gym>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.17.3)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: tensorflow~=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (2.8.0+zzzcolab20220506162203)\n",
            "Requirement already satisfied: dm-reverb~=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.7.3)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.7/dist-packages (from dm-reverb~=0.7.0->tf-agents[reverb]) (1.3.9)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from dm-reverb~=0.7.0->tf-agents[reverb]) (0.1.7)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents[reverb]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents[reverb]) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17.0->tf-agents[reverb]) (0.16.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (57.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (1.46.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (0.5.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (0.26.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (14.0.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (1.1.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (1.1.2)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (2.8.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-agents[reverb]) (2.8.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow~=2.8.0->tf-agents[reverb]) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.8.0->tf-agents[reverb]) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (3.3.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-agents[reverb]) (3.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents[reverb]) (4.4.2)\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "!pip install 'imageio==2.4.0'\n",
        "# !pip install pyvirtualdisplay\n",
        "!pip install tf-agents[reverb]\n",
        "# !pip install pyglet\n",
        "# !pip install gym\n",
        "# !pip install keras-rl2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4jQh86-zm1d"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "# import pyvirtualdisplay\n",
        "# import reverb\n",
        "import gym\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "from typing import Optional, Sequence, Text, cast\n",
        "\n",
        "import gin\n",
        "import tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from tf_agents.policies import tf_policy\n",
        "from tf_agents.trajectories import policy_step\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import batched_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.metrics import py_metrics\n",
        "from tf_agents.networks import network\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.policies import greedy_policy, epsilon_greedy_policy\n",
        "from tf_agents.policies import q_policy\n",
        "from tf_agents.policies import tf_policy\n",
        "from tf_agents.policies import utils as policy_utilities\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tf_agents.typing import types\n",
        "from tf_agents.utils import nest_utils\n",
        "from tf_agents.distributions import masked\n",
        "from tf_agents.specs import bandit_spec_utils\n",
        "\n",
        "# from rl.agents import DQNAgent\n",
        "# from rl.policy import BoltzmannQPolicy\n",
        "# from rl.memory import SequentialMemory\n",
        "from gym import spaces\n",
        "from deck import Deck\n",
        "from unoplayer import UnoPlayer\n",
        "from game import Game\n",
        "from scoreboard import Scoreboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jqoo36VPTxd"
      },
      "source": [
        "# **Creating Custom RL Environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdxVNUy_WEHy"
      },
      "outputs": [],
      "source": [
        "PRINT_VERBOSE = True\n",
        "PLAYER_FILENAME = \"players.txt\"\n",
        "player_names = []\n",
        "player_classes = []\n",
        "GAMES = 2\n",
        "players = 0\n",
        "rounds_played = 0\n",
        "rounds_won = 0\n",
        "\n",
        "\n",
        "def load_player_data():\n",
        "    r = open(PLAYER_FILENAME, 'r')\n",
        "    line = r.readline().strip().split(',')\n",
        "    global players\n",
        "    while line[0]:\n",
        "        player_names.append(line[0])\n",
        "        player_classes.append(line[1] + \"_unoplayer.\" + line[1] + \"_UnoPlayer\")\n",
        "        line = r.readline().strip().split(',')\n",
        "        players += 1\n",
        "\n",
        "load_player_data()\n",
        "s = Scoreboard(player_names.copy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnOaU6RmIRCA"
      },
      "outputs": [],
      "source": [
        "# class UnoEnv(gym.Env):\n",
        "#   def __init__(self):\n",
        "#     super(UnoEnv, self).__init__()\n",
        "\n",
        "#     # action space = 10 number cards * 4 colors + 3 types of special cards (draw 2, reverse, skip) * 4 colors + 2 types of wild 4 cards * 4 colors \n",
        "#     self.action_space = spaces.Discrete(60)\n",
        "\n",
        "#     # observation space = deck + player's hand, 54 possible cards each + num cards in others' deck\n",
        "#     # for normalization purposes, each card is represented as 1/max_single_type\n",
        "#     self.max_single_type = max(Deck.NUMBER_OF_DUP_REGULAR_CARDS, Deck.NUMBER_OF_DUP_ZERO_CARDS, Deck.NUMBER_OF_DUP_SPECIAL_CARDS, Deck.NUMBER_OF_WILD_CARDS, Deck.NUMBER_OF_WILD_D4_CARDS)\n",
        "#     self.max_points = Deck.NUMBER_OF_DUP_REGULAR_CARDS * 200 + Deck.NUMBER_OF_DUP_SPECIAL_CARDS * 240 + Deck.NUMBER_OF_WILD_CARDS * 50 + Deck.NUMBER_OF_WILD_D4_CARDS * 50\n",
        "#     self.num_cards = Deck.NUMBER_OF_DUP_REGULAR_CARDS * 36 + Deck.NUMBER_OF_DUP_ZERO_CARDS * 4 + Deck.NUMBER_OF_DUP_SPECIAL_CARDS * 12 + Deck.NUMBER_OF_WILD_CARDS + Deck.NUMBER_OF_WILD_D4_CARDS\n",
        "\n",
        "#     self.g = Game(s, player_classes)\n",
        "#     self.agent_index = -10000\n",
        "#     for i in range(len(self.g.h)):\n",
        "#       if self.g.scoreboard.get_player_list()[i] == \"dqn\":\n",
        "#         self.agent_index = i\n",
        "#         break\n",
        "#     else:\n",
        "#       raise ValueError(\"'dqn' not found in player list!\")\n",
        "\n",
        "#     self.observation_shape = 108 + self.g.scoreboard.get_num_players() - 1\n",
        "#     self.observation_space = spaces.Box(low=np.zeros(self.observation_shape), high=np.ones(self.observation_shape))\n",
        "\n",
        "#   def step(self, action):\n",
        "#     # make agent make move according to implicit policy\n",
        "#     game_end = False\n",
        "#     agent_win = False\n",
        "#     while self.g.h[self.g.curr_player].get_player_name() != \"dqn\":\n",
        "#       game_end = self.g.make_move()\n",
        "#       if game_end:\n",
        "#         break\n",
        "\n",
        "#     if not game_end:\n",
        "#       game_end = self.g.make_move()\n",
        "#       if game_end:\n",
        "#         agent_win = True\n",
        "    \n",
        "#     if game_end:\n",
        "#       if agent_win:\n",
        "#         return self.make_observation(), self.g.round_points/self.max_points, True, {}\n",
        "#       else:\n",
        "#         return self.make_observation(), -1 * self.g.round_points/self.max_points, True, {}\n",
        "#     else:\n",
        "#       return self.make_observation(), 0, False, {}\n",
        "\n",
        "#     # then, have all other players make turns\n",
        "#     # if game doesn't end return 0\n",
        "#     # if game ends and agent wins, reward is normalized amount of points earned\n",
        "#     # if game ends and agent loses, reward is normalized amount of points earned by winner\n",
        "\n",
        "#     # returns: state, reward, termination status, dict\n",
        "  \n",
        "#   def render(self):\n",
        "#     # no graphical rendering\n",
        "#     pass\n",
        "\n",
        "#   def reset(self):\n",
        "#     self.g = Game(s, player_classes)\n",
        "#     self.agent_index = -10000\n",
        "#     for i in range(len(self.g.h)):\n",
        "#       if self.g.scoreboard.get_player_list()[i] == \"dqn\":\n",
        "#         self.agent_index = i\n",
        "#         break\n",
        "#     else:\n",
        "#       raise ValueError(\"'dqn' not found in player list!\")\n",
        "\n",
        "#     # returns: state/observation\n",
        "#     return self.make_observation()\n",
        "  \n",
        "#   def make_observation(self):\n",
        "#     out = [0] * 108\n",
        "\n",
        "#     # check for cards in hand\n",
        "#     for i in self.g.h[self.agent_index].cards:\n",
        "#       if i.color == UnoPlayer.Color.NONE:\n",
        "#         if i.rank == UnoPlayer.Rank.WILD:\n",
        "#           out[52] += 1/self.max_single_type\n",
        "#         if i.rank == UnoPlayer.Rank.WILD_D4:\n",
        "#           out[53] += 1/self.max_single_type\n",
        "#       else:\n",
        "#         m = -10000\n",
        "#         if i.rank == UnoPlayer.Rank.NUMBER:\n",
        "#           m = i.number\n",
        "#         if i.rank == UnoPlayer.Rank.SKIP:\n",
        "#           m = 10\n",
        "#         if i.rank == UnoPlayer.Rank.REVERSE:\n",
        "#           m = 11\n",
        "#         if i.rank == UnoPlayer.Rank.DRAW_TWO:\n",
        "#           m = 12\n",
        "#         if i.color == UnoPlayer.Color.RED:\n",
        "#           out[m * 4] += 1/self.max_single_type\n",
        "#         if i.color == UnoPlayer.Color.YELLOW:\n",
        "#           out[m * 4 + 1] += 1/self.max_single_type\n",
        "#         if i.color == UnoPlayer.Color.BLUE:\n",
        "#           out[m * 4 + 2] += 1/self.max_single_type\n",
        "#         if i.color == UnoPlayer.Color.GREEN:\n",
        "#           out[m * 4 + 3] += 1/self.max_single_type\n",
        "\n",
        "#     # check for cards in discard pile\n",
        "#     for i in self.g.get_game_state().get_played_cards():\n",
        "#       if i.color == UnoPlayer.Color.NONE:\n",
        "#         if i.rank == UnoPlayer.Rank.WILD:\n",
        "#           out[106] += 1/self.max_single_type\n",
        "#         if i.rank == UnoPlayer.Rank.WILD_D4:\n",
        "#           out[107] += 1/self.max_single_type\n",
        "#       else:\n",
        "#         m = -10000\n",
        "#         if i.rank == UnoPlayer.Rank.NUMBER:\n",
        "#           m = i.number\n",
        "#         if i.rank == UnoPlayer.Rank.SKIP:\n",
        "#           m = 10\n",
        "#         if i.rank == UnoPlayer.Rank.REVERSE:\n",
        "#           m = 11\n",
        "#         if i.rank == UnoPlayer.Rank.DRAW_TWO:\n",
        "#           m = 12\n",
        "#         if i.color == UnoPlayer.Color.RED:\n",
        "#           out[m * 4 + 54] += 1/self.max_single_type\n",
        "#         if i.color == UnoPlayer.Color.YELLOW:\n",
        "#           out[m * 4 + 55] += 1/self.max_single_type\n",
        "#         if i.color == UnoPlayer.Color.BLUE:\n",
        "#           out[m * 4 + 56] += 1/self.max_single_type\n",
        "#         if i.color == UnoPlayer.Color.GREEN:\n",
        "#           out[m * 4 + 57] += 1/self.max_single_type\n",
        "\n",
        "#     # get amount of cards in other's hands\n",
        "#     # for (i, j) in enumerate(self.g.get_game_state().get_num_cards_in_hands_of_upcoming_players()):\n",
        "#     #   if i != self.agent_index:\n",
        "#     #     out[108 + i] = j/self.num_cards\n",
        "    \n",
        "#     upcoming = self.g.get_game_state().get_num_cards_in_hands_of_upcoming_players()[:-1]\n",
        "#     out += upcoming\n",
        "    \n",
        "#     return np.array(out)\n",
        "  \n",
        "#   def check_legal_action(self, action):\n",
        "#     hand = self.make_observation()[:54]\n",
        "#     if action < 52:\n",
        "#       return hand[action] != 0\n",
        "#     else:\n",
        "#       if action < 56:\n",
        "#         return hand[52] != 0\n",
        "#       else:\n",
        "#         return hand[53] != 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-06L4JWFY7G"
      },
      "outputs": [],
      "source": [
        "class UnoEnv(py_environment.PyEnvironment):\n",
        "  def __init__(self):\n",
        "    # action space = 10 number cards * 4 colors + 3 types of special cards (draw 2, reverse, skip) * 4 colors + 2 types of wild 4 cards * 4 colors \n",
        "    self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int32, minimum=0, maximum=60, name='action')\n",
        "\n",
        "    # observation space = deck + player's hand, 54 possible cards each + num cards in others' deck\n",
        "    # for normalization purposes, each card is represented as 1/max_single_type\n",
        "    self.max_single_type = max(Deck.NUMBER_OF_DUP_REGULAR_CARDS, Deck.NUMBER_OF_DUP_ZERO_CARDS, Deck.NUMBER_OF_DUP_SPECIAL_CARDS, Deck.NUMBER_OF_WILD_CARDS, Deck.NUMBER_OF_WILD_D4_CARDS)\n",
        "    self.max_points = Deck.NUMBER_OF_DUP_REGULAR_CARDS * 200 + Deck.NUMBER_OF_DUP_SPECIAL_CARDS * 240 + Deck.NUMBER_OF_WILD_CARDS * 50 + Deck.NUMBER_OF_WILD_D4_CARDS * 50\n",
        "    self.num_cards = Deck.NUMBER_OF_DUP_REGULAR_CARDS * 36 + Deck.NUMBER_OF_DUP_ZERO_CARDS * 4 + Deck.NUMBER_OF_DUP_SPECIAL_CARDS * 12 + Deck.NUMBER_OF_WILD_CARDS + Deck.NUMBER_OF_WILD_D4_CARDS\n",
        "\n",
        "    self.g = Game(s, player_classes)\n",
        "    self.agent_index = -10000\n",
        "    for i in range(len(self.g.h)):\n",
        "      if self.g.scoreboard.get_player_list()[i] == \"dqn\":\n",
        "        self.agent_index = i\n",
        "        break\n",
        "    else:\n",
        "      raise ValueError(\"'dqn' not found in player list!\")\n",
        "\n",
        "    self.observation_shape = 108 + self.g.scoreboard.get_num_players() - 1\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(shape=(self.observation_shape,), dtype=np.float32, minimum=0, maximum=1, name='observation')\n",
        "    self._state = self.make_observation()\n",
        "    self._episode_ended = False\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "  \n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _step(self, action):\n",
        "    # print(\"DEBUG IN STEP - AGENT INDEX:\", self.agent_index)\n",
        "    # print(\"DEBUG - STEP TAKEN\")\n",
        "    if self._episode_ended:\n",
        "      return self.reset()\n",
        "\n",
        "    # print(\"DEBUG IN STEP - AGENT INDEX 2:\", self.agent_index)\n",
        "    # make agent make move according to implicit policy\n",
        "    self._episode_ended = False\n",
        "    agent_win = False\n",
        "    # while self.g.h[self.g.curr_player].get_player_name() != \"dqn\":\n",
        "    #   self._episode_ended = self.g.make_move()\n",
        "    #   if self._episode_ended:\n",
        "    #     break\n",
        "\n",
        "    self._episode_ended = self.g.make_move(action)\n",
        "    if self._episode_ended:\n",
        "      agent_win = True\n",
        "    \n",
        "    if not agent_win:\n",
        "      while self.g.h[self.g.curr_player].get_player_name() != \"dqn\":\n",
        "        self._episode_ended = self.g.make_move()\n",
        "        if self._episode_ended:\n",
        "          break\n",
        "    \n",
        "    if self._episode_ended:\n",
        "      global rounds_played\n",
        "      rounds_played += 1\n",
        "      if agent_win:\n",
        "        global rounds_won\n",
        "        rounds_won += 1\n",
        "        # self._current_time_step = ts.termination(tf.convert_to_tensor(self.make_observation()), self.g.round_points/self.max_points)\n",
        "        self._current_time_step = ts.termination(tf.convert_to_tensor(self.make_observation()), self.g.round_points/self.max_points)\n",
        "        return self._current_time_step\n",
        "      else:\n",
        "        self._current_time_step = ts.termination(tf.convert_to_tensor(self.make_observation()), -1 * self.g.round_points/self.max_points)\n",
        "        return self._current_time_step\n",
        "    else:\n",
        "      self._current_time_step = ts.transition(tf.convert_to_tensor(self.make_observation()), reward=0.0, discount=0.9)\n",
        "      return self._current_time_step\n",
        "\n",
        "    # then, have all other players make turns\n",
        "    # if game doesn't end return 0\n",
        "    # if game ends and agent wins, reward is normalized amount of points earned\n",
        "    # if game ends and agent loses, reward is normalized amount of points earned by winner\n",
        "\n",
        "  def _reset(self):\n",
        "    # print(\"DEBUG - RESET\")\n",
        "    self.g = Game(s, player_classes)\n",
        "    self.agent_index = -10000\n",
        "    for i in range(len(self.g.h)):\n",
        "      if self.g.scoreboard.get_player_list()[i] == \"dqn\":\n",
        "        self.agent_index = i\n",
        "        break\n",
        "    else:\n",
        "      raise ValueError(\"'dqn' not found in player list!\")\n",
        "\n",
        "    while self.g.h[self.g.curr_player].get_player_name() != \"dqn\":\n",
        "      self._episode_ended = self.g.make_move()\n",
        "      if self._episode_ended:\n",
        "        break\n",
        "\n",
        "    self._state = self.make_observation()\n",
        "    # print(\"DEBUG - RESET STATE:\", self._state)\n",
        "    # print(\"DEBUG - RESET PLAYER 0\", self.g.h[0])\n",
        "    self._episode_ended = False\n",
        "\n",
        "    # returns: state/observation\n",
        "    self._current_time_step = ts.restart(tf.convert_to_tensor(self._state))\n",
        "    return self._current_time_step\n",
        "  \n",
        "  def make_observation(self):\n",
        "    out = [0] * 108\n",
        "\n",
        "    #for i, j in enumerate(self.g.h):\n",
        "      # print(\"DEBUG - PLAYER\", i, \"CARDS:\", j)\n",
        "\n",
        "    # check for cards in hand\n",
        "    for i in self.g.h[self.agent_index].cards:\n",
        "      if i.color == UnoPlayer.Color.NONE:\n",
        "        if i.rank == UnoPlayer.Rank.WILD:\n",
        "          out[52] += 1/self.max_single_type\n",
        "        if i.rank == UnoPlayer.Rank.WILD_D4:\n",
        "          out[53] += 1/self.max_single_type\n",
        "      else:\n",
        "        m = -10000\n",
        "        if i.rank == UnoPlayer.Rank.NUMBER:\n",
        "          m = i.number\n",
        "        if i.rank == UnoPlayer.Rank.SKIP:\n",
        "          m = 10\n",
        "        if i.rank == UnoPlayer.Rank.REVERSE:\n",
        "          m = 11\n",
        "        if i.rank == UnoPlayer.Rank.DRAW_TWO:\n",
        "          m = 12\n",
        "        if i.color == UnoPlayer.Color.RED:\n",
        "          out[m * 4] += 1/self.max_single_type\n",
        "        if i.color == UnoPlayer.Color.YELLOW:\n",
        "          out[m * 4 + 1] += 1/self.max_single_type\n",
        "        if i.color == UnoPlayer.Color.BLUE:\n",
        "          out[m * 4 + 2] += 1/self.max_single_type\n",
        "        if i.color == UnoPlayer.Color.GREEN:\n",
        "          out[m * 4 + 3] += 1/self.max_single_type\n",
        "\n",
        "    # print(\"DEBUG - DECK:\", self.g.deck)\n",
        "    # check for cards in discard pile\n",
        "    for i in self.g.get_game_state().get_played_cards():\n",
        "      if i.color == UnoPlayer.Color.NONE:\n",
        "        if i.rank == UnoPlayer.Rank.WILD:\n",
        "          out[106] += 1/self.max_single_type\n",
        "        if i.rank == UnoPlayer.Rank.WILD_D4:\n",
        "          out[107] += 1/self.max_single_type\n",
        "      else:\n",
        "        m = -10000\n",
        "        if i.rank == UnoPlayer.Rank.NUMBER:\n",
        "          m = i.number\n",
        "        if i.rank == UnoPlayer.Rank.SKIP:\n",
        "          m = 10\n",
        "        if i.rank == UnoPlayer.Rank.REVERSE:\n",
        "          m = 11\n",
        "        if i.rank == UnoPlayer.Rank.DRAW_TWO:\n",
        "          m = 12\n",
        "        if i.color == UnoPlayer.Color.RED:\n",
        "          out[m * 4 + 54] += 1/self.max_single_type\n",
        "        if i.color == UnoPlayer.Color.YELLOW:\n",
        "          out[m * 4 + 55] += 1/self.max_single_type\n",
        "        if i.color == UnoPlayer.Color.BLUE:\n",
        "          out[m * 4 + 56] += 1/self.max_single_type\n",
        "        if i.color == UnoPlayer.Color.GREEN:\n",
        "          out[m * 4 + 57] += 1/self.max_single_type\n",
        "\n",
        "    # get amount of cards in other's hands\n",
        "    # for (i, j) in enumerate(self.g.get_game_state().get_num_cards_in_hands_of_upcoming_players()):\n",
        "    #   if i != self.agent_index:\n",
        "    #     out[108 + i] = j/self.num_cards\n",
        "    \n",
        "    upcoming = self.g.get_game_state().get_num_cards_in_hands_of_upcoming_players()[:-1]\n",
        "    upcoming = [i/self.num_cards for i in upcoming]\n",
        "    out += upcoming\n",
        "    \n",
        "    return np.array(out, dtype=np.float32)\n",
        "  \n",
        "  def check_legal_action(self, action):\n",
        "    # print(\"DEBUG - AGENT INDEX:\", self.agent_index)\n",
        "    # print(\"DEBUG - ACTION: \", action)\n",
        "    hand = self.make_observation()[:54]\n",
        "    # print(\"DEBUG - HAND: \", hand)\n",
        "\n",
        "    if action <= -1 or action >= 60:\n",
        "      return True\n",
        "\n",
        "    up_card_int = -1\n",
        "    up_card = self.g.up_card\n",
        "    # print(\"DEBUG - UP CARD:\", up_card)\n",
        "\n",
        "    if up_card.color == UnoPlayer.Color.NONE:\n",
        "      if up_card.rank == UnoPlayer.Rank.WILD:\n",
        "        up_card_int = 52\n",
        "      if up_card.rank == UnoPlayer.Rank.WILD_D4:\n",
        "        up_card_int = 53\n",
        "    else:\n",
        "      m = -10000\n",
        "      if up_card.rank == UnoPlayer.Rank.NUMBER:\n",
        "        m = up_card.number\n",
        "      if up_card.rank == UnoPlayer.Rank.SKIP:\n",
        "        m = 10\n",
        "      if up_card.rank == UnoPlayer.Rank.REVERSE:\n",
        "        m = 11\n",
        "      if up_card.rank == UnoPlayer.Rank.DRAW_TWO:\n",
        "        m = 12\n",
        "      if up_card.color == UnoPlayer.Color.RED:\n",
        "        up_card_int = m * 4\n",
        "      if up_card.color == UnoPlayer.Color.YELLOW:\n",
        "        up_card_int = m * 4 + 1\n",
        "      if up_card.color == UnoPlayer.Color.BLUE:\n",
        "        up_card_int = m * 4 + 2\n",
        "      if up_card.color == UnoPlayer.Color.GREEN:\n",
        "        up_card_int = m * 4 + 3\n",
        "    \n",
        "    # print(\"DEBUG - UP CARD INT:\", up_card_int)\n",
        "\n",
        "    if not self.can_play_on(action, up_card_int, self.g.called_color):\n",
        "      # print(\"DEBUG - ACTION INVALID A\")\n",
        "      return False\n",
        "\n",
        "    if action < 52:\n",
        "      # print(\"DEBUG - ACTION INVALID B\")\n",
        "      return hand[action] != 0\n",
        "    else:\n",
        "      if action < 56:\n",
        "        # print(\"DEBUG - ACTION INVALID C\")\n",
        "        return hand[52] != 0\n",
        "      else:\n",
        "        # print(\"DEBUG - ACTION INVALID D\")\n",
        "        return hand[53] != 0\n",
        "  \n",
        "  def state(self):\n",
        "    return self._state\n",
        "  \n",
        "  def can_play_on(self, index, up_card, called_color):\n",
        "    result = index >= 52\n",
        "    # print(\"DEBUG - CAN PLAY ON A\", result)\n",
        "    result = result or index % 4 == up_card % 4\n",
        "    # print(\"DEBUG - CAN PLAY ON B\", result)\n",
        "    if called_color == UnoPlayer.Color.RED:\n",
        "      result = result or index % 4 == 0\n",
        "      # print(\"DEBUG - CAN PLAY ON C\", result)\n",
        "    elif called_color == UnoPlayer.Color.YELLOW:\n",
        "      result = result or index % 4 == 1\n",
        "      # print(\"DEBUG - CAN PLAY ON D\", result)\n",
        "    elif called_color == UnoPlayer.Color.BLUE:\n",
        "      result = result or index % 4 == 2\n",
        "      # print(\"DEBUG - CAN PLAY ON E\", result)\n",
        "    elif called_color == UnoPlayer.Color.GREEN:\n",
        "      result = result or index % 4 == 3\n",
        "      # print(\"DEBUG - CAN PLAY ON F\", result)\n",
        "    result = result or (index // 4 == up_card // 4 and index >= 40)\n",
        "    # print(\"DEBUG - CAN PLAY ON G\", result)\n",
        "    result = result or (index // 4 == up_card // 4 and index < 40 and up_card < 40)\n",
        "    # print(\"DEBUG - CAN PLAY ON H\", result)\n",
        "    return result\n",
        "    # result = card.get_rank() == self.Rank.WILD\n",
        "    # result = result or card.get_rank() == self.Rank.WILD_D4\n",
        "    # result = result or card.get_color() == up_card.get_color()\n",
        "    # result = result or card.get_color() == called_color\n",
        "    # result = result or ((card.get_rank() == up_card.get_rank()) and (card.get_rank() != self.Rank.NUMBER))\n",
        "    # result = result or (card.get_number() == up_card.get_number() and card.get_rank() == self.Rank.NUMBER and up_card.get_rank() == self.Rank.NUMBER)\n",
        "    # return result\n",
        "  \n",
        "  # def time_step_spec(self) -> ts.TimeStep:\n",
        "  #   \"\"\"Describes the `TimeStep` fields returned by `step()`.\n",
        "  #   Override this method to define an environment that uses non-standard values\n",
        "  #   for any of the items returned by `step()`. For example, an environment with\n",
        "  #   array-valued rewards.\n",
        "  #   Returns:\n",
        "  #     A `TimeStep` namedtuple containing (possibly nested) `ArraySpec`s defining\n",
        "  #     the step_type, reward, discount, and observation structure.\n",
        "  #   \"\"\"\n",
        "  #   print(\"DEBUG - ENV TIME STEP SPEC:\", self.observation_spec(), self.reward_spec())\n",
        "  #   return ts.time_step_spec(self.observation_spec(), self.reward_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrUwr2rsqFxs"
      },
      "source": [
        "# **Setting up DQN RL Agent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOCRjpm8TC3E"
      },
      "outputs": [],
      "source": [
        "# credits to Nicholas Renotte\n",
        "\n",
        "# def build_model(s=111, a=60):\n",
        "#   model = Sequential()\n",
        "#   model.add(tf.keras.Input(shape=(s,)))    \n",
        "#   model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "#   model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "#   model.add(tf.keras.layers.Dense(a, activation='linear'))\n",
        "#   return model\n",
        "\n",
        "# model = build_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWyCyhQgT2PV"
      },
      "outputs": [],
      "source": [
        "class MaxLegalGreedyPolicy (greedy_policy.GreedyPolicy):\n",
        "  def __init__(self, env, policy: tf_policy.TFPolicy, name: Optional[Text] = None):\n",
        "    super().__init__(policy, name)\n",
        "    self.env = env\n",
        "\n",
        "  def _distribution(self, time_step, policy_state):\n",
        "    def dist_fn(dist):\n",
        "      try:\n",
        "        # print(tf.nest.flatten(tf.cast(dist.logits, dtype=np.float32)))\n",
        "        dists = np.array(tf.cast(dist.logits, dtype=np.float32))\n",
        "\n",
        "        actions = []\n",
        "        for d in dists:\n",
        "          # d = np.ravel(d)\n",
        "          \n",
        "          # print(\"DEBUG - DISTRIBUTION LOGITS:\", d)\n",
        "          # print(\"DEBUG - DISTRIBUTION LOGITS 0:\", d[0])\n",
        "\n",
        "          # print(\"DEBUG - DISTRIBUTION LOGITS ARGMAX:\", tf.argmax(d))\n",
        "          action = int(np.argmax(d[:60]))\n",
        "          genv = self.env.pyenv.envs[0]\n",
        "          # print(\"DEBUG - ENV TYPE\", type(self.env))\n",
        "          # print(\"DEBUG - PYENV TYPE\", type(self.env.pyenv))\n",
        "          while not genv.check_legal_action(action):\n",
        "            # print(\"DEBUG - MODIFIFED DISTRIBUTION:\", d[0])\n",
        "            # print(\"DEBUG - DISTRIBUTION MAX:\", np.max(d[0]), int(np.max(d[0])) == -1000000)\n",
        "            d[action] = -1000000\n",
        "            action = int(np.argmax(d[:60]))\n",
        "            if int(np.max(d[:60])) == -1000000:\n",
        "              action = 60\n",
        "              break\n",
        "          actions.append(action)\n",
        "\n",
        "\n",
        "      except NotImplementedError:\n",
        "        raise ValueError(\"Your network's distribution does not implement mode \"\n",
        "                         'making it incompatible with a greedy policy.'\n",
        "                        ) from NotImplementedError\n",
        "\n",
        "      # print(\"DEBUG - DETERMINISTIC:\", tfp.distributions.Deterministic(loc=action))\n",
        "      # print(\"DEBUG - DISTRIBUTION ACTIONS\", actions)\n",
        "      # print(\"DEBUG - DETERMINISTIC:\", tfp.distributions.Deterministic(loc=tf.constant(actions, dtype=tf.int32)))\n",
        "      return tfp.distributions.Deterministic(loc=tf.constant(actions, dtype=tf.int32))\n",
        "\n",
        "    # print(\"DEBUG - OBSERVATION:\", time_step.observation)\n",
        "    # print(\"DEBUG - POLICY STATE:\", policy_state)\n",
        "    # print(\"DEBUG - DISTRIBUTION TIME STEP\", time_step)\n",
        "    distribution_step = self.wrapped_policy.distribution(\n",
        "        time_step, policy_state)\n",
        "    # print(\"DEBUG - DISTRIBUTION STEP\", distribution_step)\n",
        "    # print(\"DEBUG - DISTRIBUTION STEP ACTION PROBS: \", distribution_step.action)\n",
        "    # print(\"DEBUG - POLICY STEP ACTION:\", policy_step.PolicyStep(\n",
        "    #     tf.nest.map_structure(dist_fn, distribution_step.action),\n",
        "    #     distribution_step.state, distribution_step.info).action.dtype)\n",
        "    # print(\"DEBUG - DISTRIBUTION STEP ACTION\", tf.cast(tf.nest.map_structure(dist_fn, distribution_step.action), tf.int32))\n",
        "    return policy_step.PolicyStep(\n",
        "        tf.nest.map_structure(dist_fn, distribution_step.action),\n",
        "        distribution_step.state, distribution_step.info)\n",
        "  \n",
        "  # def _get_initial_state(self, batch_size: int) -> types.NestedTensor:\n",
        "  #   \"\"\"Returns the initial state of the policy network.\n",
        "  #   Args:\n",
        "  #     batch_size: A constant or Tensor holding the batch size. Can be None, in\n",
        "  #       which case the state will not have a batch dimension added.\n",
        "  #   Returns:\n",
        "  #     A nest of zero tensors matching the spec of the policy network state.\n",
        "  #   \"\"\"\n",
        "  #   # return tensor_spec.zero_spec_nest(\n",
        "  #   #     tf.TensorSpec(shape=(1,), dtype=tf.float32),\n",
        "  #   #     outer_dims=None if batch_size is None else [batch_size])\n",
        "  #   return tensor_spec.zero_spec_nest(\n",
        "  #       self._policy_state_spec,\n",
        "  #       outer_dims=None if batch_size is None else [batch_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5i_TzoJBUk_W"
      },
      "outputs": [],
      "source": [
        "class LegalRandomTFPolicy (random_tf_policy.RandomTFPolicy):\n",
        "  def __init__(self, time_step_spec: ts.TimeStep,\n",
        "               action_spec: types.NestedTensorSpec, genv, *args, **kwargs):\n",
        "    self.genv = genv\n",
        "    super().__init__(time_step_spec, action_spec, *args, **kwargs)\n",
        "\n",
        "  def _action(self, time_step, policy_state, seed):\n",
        "    observation_and_action_constraint_splitter = (\n",
        "        self.observation_and_action_constraint_splitter)\n",
        "\n",
        "    outer_dims = nest_utils.get_outer_shape(time_step, self._time_step_spec)\n",
        "    if observation_and_action_constraint_splitter is not None:\n",
        "      observation, mask = observation_and_action_constraint_splitter(\n",
        "          time_step.observation)\n",
        "\n",
        "      if self._stationary_mask is not None:\n",
        "        mask = mask * self._stationary_mask\n",
        "\n",
        "      action_spec = tensor_spec.from_spec(self.action_spec)\n",
        "      action_spec = cast(tensor_spec.BoundedTensorSpec, action_spec)\n",
        "      zero_logits = tf.cast(tf.zeros_like(mask), tf.float32)\n",
        "      masked_categorical = masked.MaskedCategorical(zero_logits, mask)\n",
        "      action_ = tf.cast(masked_categorical.sample() + action_spec.minimum,\n",
        "                        action_spec.dtype)\n",
        "      # print(\"DEBUG - EPSILON ACTION A:\", action_)\n",
        "\n",
        "      # If the action spec says each action should be shaped (1,), add another\n",
        "      # dimension so the final shape is (B, 1) rather than (B,).\n",
        "      if action_spec.shape.rank == 1:\n",
        "        action_ = tf.expand_dims(action_, axis=-1)\n",
        "        # print(\"DEBUG - EPSILON ACTION B:\", action_)\n",
        "      policy_info = tensor_spec.sample_spec_nest(\n",
        "          self._info_spec, outer_dims=outer_dims)\n",
        "    else:\n",
        "      observation = time_step.observation\n",
        "      action_spec = cast(tensor_spec.BoundedTensorSpec, self.action_spec)\n",
        "\n",
        "      if self._accepts_per_arm_features:\n",
        "        max_num_arms = action_spec.maximum - action_spec.minimum + 1\n",
        "        batch_size = tf.shape(time_step.step_type)[0]\n",
        "        num_actions = observation.get(\n",
        "            bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY,\n",
        "            tf.ones(shape=(batch_size,), dtype=tf.int32) * max_num_arms)\n",
        "        mask = tf.sequence_mask(num_actions, max_num_arms)\n",
        "        zero_logits = tf.cast(tf.zeros_like(mask), tf.float32)\n",
        "        masked_categorical = masked.MaskedCategorical(zero_logits, mask)\n",
        "        action_ = tf.nest.map_structure(\n",
        "            lambda t: tf.cast(masked_categorical.sample() + t.minimum, t.dtype),\n",
        "            action_spec)\n",
        "        # print(\"DEBUG - EPSILON ACTION C:\", action_)\n",
        "      elif self._stationary_mask is not None:\n",
        "        batch_size = tf.shape(time_step.step_type)[0]\n",
        "        mask = tf.tile(self._stationary_mask, [batch_size, 1])\n",
        "        zero_logits = tf.cast(tf.zeros_like(mask), tf.float32)\n",
        "        masked_categorical = masked.MaskedCategorical(zero_logits, mask)\n",
        "        action_ = tf.cast(masked_categorical.sample() + action_spec.minimum,\n",
        "                          action_spec.dtype)\n",
        "        # print(\"DEBUG - EPSILON ACTION D:\", action_)\n",
        "      else:\n",
        "        # action_ = tensor_spec.sample_spec_nest(\n",
        "        #     self._action_spec, seed=seed, outer_dims=outer_dims)\n",
        "        action = random.randint(0, self.genv.action_spec().maximum)\n",
        "        if type(self.genv) == tf_py_environment.TFPyEnvironment:\n",
        "          while not self.genv.pyenv.envs[0].check_legal_action(action):\n",
        "            action = random.randint(0, self.genv.action_spec().maximum)\n",
        "        else:\n",
        "          while not self.genv.check_legal_action(action):\n",
        "            action = random.randint(0, self.genv.action_spec().maximum)\n",
        "        action_ = tf.constant([action])\n",
        "\n",
        "      policy_info = tensor_spec.sample_spec_nest(\n",
        "          self._info_spec, outer_dims=outer_dims)\n",
        "\n",
        "    # Update policy info with chosen arm features.\n",
        "    if self._accepts_per_arm_features:\n",
        "      def _gather_fn(t):\n",
        "        return tf.gather(params=t, indices=action_, batch_dims=1)\n",
        "      chosen_arm_features = tf.nest.map_structure(\n",
        "          _gather_fn, observation[bandit_spec_utils.PER_ARM_FEATURE_KEY])\n",
        "\n",
        "      if policy_utilities.has_chosen_arm_features(self._info_spec):\n",
        "        policy_info = policy_info._replace(\n",
        "            chosen_arm_features=chosen_arm_features)\n",
        "\n",
        "    # TODO(b/78181147): Investigate why this control dependency is required.\n",
        "    def _maybe_convert_sparse_tensor(t):\n",
        "      if isinstance(t, tf.SparseTensor):\n",
        "        return tf.sparse.to_dense(t)\n",
        "      else:\n",
        "        return t\n",
        "    if time_step is not None:\n",
        "      with tf.control_dependencies(\n",
        "          tf.nest.flatten(tf.nest.map_structure(_maybe_convert_sparse_tensor,\n",
        "                                                time_step))):\n",
        "        action_ = tf.nest.map_structure(tf.identity, action_)\n",
        "        # print(\"DEBUG - EPSILON ACTION F:\", action_)\n",
        "\n",
        "    if self.emit_log_probability:\n",
        "      if (self._accepts_per_arm_features\n",
        "          or observation_and_action_constraint_splitter is not None\n",
        "          or self._stationary_mask is not None):\n",
        "        action_spec = cast(tensor_spec.BoundedTensorSpec, self.action_spec)\n",
        "        log_probability = masked_categorical.log_prob(\n",
        "            action_ - action_spec.minimum)\n",
        "      else:\n",
        "        log_probability = tf.nest.map_structure(\n",
        "            lambda s: random_tf_policy._calculate_log_probability(outer_dims, s),\n",
        "            self._action_spec)\n",
        "      policy_info = policy_step.set_log_probability(policy_info,\n",
        "                                                    log_probability)\n",
        "\n",
        "    step = policy_step.PolicyStep(action_, policy_state, policy_info)\n",
        "    return step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mjWogOHUsXu"
      },
      "outputs": [],
      "source": [
        "class MaxLegalEpsilonGreedyPolicy (epsilon_greedy_policy.EpsilonGreedyPolicy):\n",
        "  \"\"\"Returns epsilon-greedy samples of a given policy.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               policy: tf_policy.TFPolicy,\n",
        "               genv,\n",
        "               epsilon: types.FloatOrReturningFloat,\n",
        "               exploration_mask: Optional[Sequence[int]] = None,\n",
        "               info_fields_to_inherit_from_greedy: Sequence[Text] = (),\n",
        "               name: Optional[Text] = None):\n",
        "    \"\"\"Builds an epsilon-greedy MixturePolicy wrapping the given policy.\n",
        "    Args:\n",
        "      policy: A policy implementing the tf_policy.TFPolicy interface.\n",
        "      epsilon: The probability of taking the random action represented as a\n",
        "        float scalar, a scalar Tensor of shape=(), or a callable that returns a\n",
        "        float scalar or Tensor.\n",
        "      exploration_mask: A `[0, 1]` vector describing which actions should be in\n",
        "        the set of exploratory actions.\n",
        "      info_fields_to_inherit_from_greedy: A list of policy info fields which\n",
        "        should be copied over from the greedy action's info, even if the random\n",
        "        action was taken.\n",
        "      name: The name of this policy.\n",
        "    Raises:\n",
        "      ValueError: If epsilon is invalid.\n",
        "    \"\"\"\n",
        "    super(MaxLegalEpsilonGreedyPolicy, self).__init__(\n",
        "        policy,\n",
        "        epsilon,\n",
        "        exploration_mask,\n",
        "        info_fields_to_inherit_from_greedy,\n",
        "        name)\n",
        "    try:\n",
        "      observation_and_action_constraint_splitter = (\n",
        "          policy.observation_and_action_constraint_splitter)\n",
        "    except AttributeError:\n",
        "      observation_and_action_constraint_splitter = None\n",
        "    try:\n",
        "      accepts_per_arm_features = policy.accepts_per_arm_features\n",
        "    except AttributeError:\n",
        "      accepts_per_arm_features = False\n",
        "    self._greedy_policy = MaxLegalGreedyPolicy(genv, policy)\n",
        "    # print(\"DEBUG - INIT GREEDY POLICY:\", self._greedy_policy)\n",
        "    self._epsilon = epsilon\n",
        "    self._exploration_mask = exploration_mask\n",
        "    self.info_fields_to_inherit_from_greedy = info_fields_to_inherit_from_greedy\n",
        "    self._random_policy = LegalRandomTFPolicy(\n",
        "        policy.time_step_spec,\n",
        "        policy.action_spec,\n",
        "        genv,\n",
        "        emit_log_probability=policy.emit_log_probability,\n",
        "        observation_and_action_constraint_splitter=(\n",
        "            observation_and_action_constraint_splitter),\n",
        "        accepts_per_arm_features=accepts_per_arm_features,\n",
        "        stationary_mask=exploration_mask,\n",
        "        info_spec=policy.info_spec)\n",
        "\n",
        "  def _action(self, time_step, policy_state, seed):\n",
        "      new_step_type = tf.data.Dataset.from_tensors(time_step.step_type).unbatch().as_numpy_iterator()\n",
        "      new_reward = tf.data.Dataset.from_tensors(time_step.reward).unbatch().as_numpy_iterator()\n",
        "      new_discount = tf.data.Dataset.from_tensors(time_step.discount).unbatch().as_numpy_iterator()\n",
        "      new_observation = tf.data.Dataset.from_tensors(time_step.observation).unbatch().as_numpy_iterator()\n",
        "      new_ts = ts.TimeStep(step_type = tf.convert_to_tensor(list(new_step_type)[0]),\n",
        "                           reward = tf.convert_to_tensor(list(new_reward)[0]),\n",
        "                           discount = tf.convert_to_tensor(list(new_discount)[0]),\n",
        "                           observation = tf.convert_to_tensor(list(new_observation)[0]))\n",
        "      time_step = new_ts\n",
        "      # print(\"DEBUG - ACTION TIME STEP:\", time_step)\n",
        "      seed_stream = tfp.util.SeedStream(seed=seed, salt='epsilon_greedy')\n",
        "      greedy_action = self._greedy_policy.action(time_step, policy_state)\n",
        "      # print(\"DEBUG - GREEDY POLICY AND ACTION\", self._greedy_policy, greedy_action)\n",
        "      # print(\"DEBUG - RANDOM POLICY\", self._random_policy)\n",
        "      random_action = self._random_policy.action(time_step, (), seed_stream())\n",
        "\n",
        "      # print(\"DEBUG - TIME STEP\", time_step, self._time_step_spec)\n",
        "      outer_shape = nest_utils.get_outer_shape(time_step, self._time_step_spec)\n",
        "      rng = tf.random.uniform(\n",
        "          outer_shape, maxval=1.0, seed=seed_stream(), name='epsilon_rng')\n",
        "      cond = tf.greater_equal(rng, self._get_epsilon())\n",
        "\n",
        "      # Selects the action/info from the random policy with probability epsilon.\n",
        "      # TODO(b/133175894): tf.compat.v1.where only supports a condition which is\n",
        "      # either a scalar or a vector. Use tf.compat.v2 so that it can support any\n",
        "      # condition whose leading dimensions are the same as the other operands of\n",
        "      # tf.where.\n",
        "      # print(\"DEBUG - OUTER SHAPE\", outer_shape, outer_shape.shape[0])\n",
        "      outer_ndims = int(outer_shape.shape[0])\n",
        "\n",
        "      if outer_ndims >= 2:\n",
        "        raise ValueError(\n",
        "            'Only supports batched time steps with a single batch dimension')\n",
        "      # if cond:\n",
        "        # print(\"DEBUG - GREEDY ACTION\")\n",
        "      # else:\n",
        "        # print(\"DEBUG - RANDOM ACTION\")\n",
        "      # print(\"DEBUG - COND\", cond)\n",
        "      # print(\"DEBUG - G\", greedy_action.action)\n",
        "      # print(\"DEBUG - R\", random_action.action)\n",
        "      action = tf.nest.map_structure(lambda g, r: tf.where(cond, g, r),\n",
        "                                    greedy_action.action, random_action.action)\n",
        "\n",
        "      if greedy_action.info:\n",
        "        if not random_action.info:\n",
        "          raise ValueError('Incompatible info field')\n",
        "        # Note that the objects in PolicyInfo may have different shapes, so we\n",
        "        # need to call nest_utils.where() on each type of object.\n",
        "        info = tf.nest.map_structure(lambda x, y: nest_utils.where(cond, x, y),\n",
        "                                    greedy_action.info, random_action.info)\n",
        "        if self._emit_log_probability:\n",
        "          # At this point, info.log_probability contains the log prob of the\n",
        "          # action chosen, conditioned on the policy that was chosen. We want to\n",
        "          # emit the full log probability of the action, so we'll add in the log\n",
        "          # probability of choosing the policy.\n",
        "          random_log_prob = tf.nest.map_structure(\n",
        "              lambda t: tf.math.log(tf.zeros_like(t) + self._get_epsilon()),\n",
        "              info.log_probability)\n",
        "          greedy_log_prob = tf.nest.map_structure(\n",
        "              lambda t: tf.math.log(tf.ones_like(t) - self._get_epsilon()),\n",
        "              random_log_prob)\n",
        "          log_prob_of_chosen_policy = nest_utils.where(cond, greedy_log_prob,\n",
        "                                                      random_log_prob)\n",
        "          log_prob = tf.nest.map_structure(lambda a, b: a + b,\n",
        "                                          log_prob_of_chosen_policy,\n",
        "                                          info.log_probability)\n",
        "          info = policy_step.set_log_probability(info, log_prob)\n",
        "        # Overwrite bandit policy info type.\n",
        "        if policy_utilities.has_bandit_policy_type(info, check_for_tensor=True):\n",
        "          # Generate mask of the same shape as bandit_policy_type (batch_size, 1).\n",
        "          # This is the opposite of `cond`, which is 1-D bool tensor (batch_size,)\n",
        "          # that is true when greedy policy was used, otherwise `cond` is false.\n",
        "          random_policy_mask = tf.reshape(tf.logical_not(cond),\n",
        "                                          tf.shape(info.bandit_policy_type))  # pytype: disable=attribute-error\n",
        "          bandit_policy_type = policy_utilities.bandit_policy_uniform_mask(\n",
        "              info.bandit_policy_type, mask=random_policy_mask)  # pytype: disable=attribute-error\n",
        "          info = policy_utilities.set_bandit_policy_type(\n",
        "              info, bandit_policy_type)\n",
        "\n",
        "        # TODO(b/196644931): Should not assume info is a named tuple.\n",
        "        if self.info_fields_to_inherit_from_greedy:\n",
        "          info = info._replace(\n",
        "              **{\n",
        "                  f: getattr(greedy_action.info, f)\n",
        "                  for f in self.info_fields_to_inherit_from_greedy\n",
        "              })\n",
        "      else:\n",
        "        if random_action.info:\n",
        "          raise ValueError('Incompatible info field')\n",
        "        info = ()\n",
        "\n",
        "      # The state of the epsilon greedy policy is the state of the underlying\n",
        "      # greedy policy (the random policy carries no state).\n",
        "      # It is commonly assumed that the new policy state only depends only\n",
        "      # on the previous state and \"time_step\", the action (be it the greedy one\n",
        "      # or the random one) does not influence the new policy state.\n",
        "      state = greedy_action.state\n",
        "\n",
        "      return policy_step.PolicyStep(action, state, info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbN555eeUxzx"
      },
      "outputs": [],
      "source": [
        "class ModifiedQNetwork(q_network.QNetwork):\n",
        "  \n",
        "\n",
        "  def call(self, observation, step_type=None, network_state=(), training=False):\n",
        "    \"\"\"Runs the given observation through the network.\n",
        "\n",
        "    Args:\n",
        "      observation: The observation to provide to the network.\n",
        "      step_type: The step type for the given observation. See `StepType` in\n",
        "        time_step.py.\n",
        "      network_state: A state tuple to pass to the network, mainly used by RNNs.\n",
        "      training: Whether the output is being used for training.\n",
        "\n",
        "    Returns:\n",
        "      A tuple `(logits, network_state)`.\n",
        "    \"\"\"\n",
        "    state, network_state = self._encoder(\n",
        "        observation, step_type=step_type, network_state=network_state,\n",
        "        training=training)\n",
        "    # for i in state[0]:\n",
        "    #   print(\"DEBUG - DTYPE\", i.dtype)\n",
        "    state = tf.reshape(state, (state.numpy().flatten().shape[-1]//self._encoder.layers[-1].units, state.shape[-1]))\n",
        "    q_value = self._q_value_layer(state, training=training)\n",
        "    return q_value, network_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzvfUrpaMUEW"
      },
      "outputs": [],
      "source": [
        "class MaxLegalDqnAgent(dqn_agent.DqnAgent):\n",
        "  def __init__(self, \n",
        "               genv, \n",
        "               time_step_spec, \n",
        "               action_spec, \n",
        "               q_network, \n",
        "               optimizer, \n",
        "               td_errors_loss_fn, \n",
        "               train_step_counter):\n",
        "    self.genv = genv\n",
        "    super().__init__(time_step_spec=time_step_spec, \n",
        "                     action_spec=action_spec, \n",
        "                     q_network=q_network, \n",
        "                     optimizer=optimizer, \n",
        "                     td_errors_loss_fn=td_errors_loss_fn,\n",
        "                     train_step_counter=train_step_counter)\n",
        "\n",
        "  def _setup_policy(self, time_step_spec, action_spec,\n",
        "                    boltzmann_temperature, emit_log_probability):\n",
        "\n",
        "    policy = q_policy.QPolicy(\n",
        "        time_step_spec,\n",
        "        action_spec,\n",
        "        q_network=self._q_network,\n",
        "        emit_log_probability=emit_log_probability,\n",
        "        observation_and_action_constraint_splitter=(\n",
        "            self._observation_and_action_constraint_splitter))\n",
        "    \n",
        "    # print(policy.policy_state_spec)\n",
        "\n",
        "    # if boltzmann_temperature is not None:\n",
        "    #   collect_policy = boltzmann_policy.BoltzmannPolicy(\n",
        "    #       policy, temperature=self._boltzmann_temperature)\n",
        "    # else:\n",
        "    #   collect_policy = MaxLegalEpsilonGreedyPolicy(\n",
        "    #       policy, genv=genv, epsilon=self._epsilon_greedy)\n",
        "    collect_policy = MaxLegalEpsilonGreedyPolicy(\n",
        "          policy, genv=self.genv, epsilon=self._epsilon_greedy)\n",
        "    # collect_policy = MaxLegalEpsilonGreedyPolicy(\n",
        "    #       policy, genv=self.genv, epsilon=1)\n",
        "    policy = MaxLegalGreedyPolicy(self.genv, policy)\n",
        "\n",
        "    # Create self._target_greedy_policy in order to compute target Q-values.\n",
        "    target_policy = q_policy.QPolicy(\n",
        "        time_step_spec,\n",
        "        action_spec,\n",
        "        q_network=self._target_q_network,\n",
        "        observation_and_action_constraint_splitter=(\n",
        "            self._observation_and_action_constraint_splitter))\n",
        "    self._target_greedy_policy = MaxLegalGreedyPolicy(self.genv, target_policy)\n",
        "\n",
        "    return policy, collect_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62A-ofq6r6GE"
      },
      "outputs": [],
      "source": [
        "train_env = tf_py_environment.TFPyEnvironment(UnoEnv())\n",
        "eval_env = tf_py_environment.TFPyEnvironment(UnoEnv())\n",
        "env = UnoEnv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7LqHtz4p6Du"
      },
      "outputs": [],
      "source": [
        "input_tensor_spec = tensor_spec.TensorSpec((108 + players,), tf.float32)\n",
        "time_step_spec = ts.time_step_spec(input_tensor_spec)\n",
        "action_spec = tensor_spec.BoundedTensorSpec((),\n",
        "                                            tf.int32,\n",
        "                                            minimum=0,\n",
        "                                            maximum=59)\n",
        "num_actions = 60\n",
        "\n",
        "# class QNetwork(network.Network):\n",
        "\n",
        "#   def __init__(self, input_tensor_spec, action_spec, num_actions=num_actions, name=None, fc_layer_params=(75, 40), activation_fn=tf.keras.activations.relu):\n",
        "#     super(QNetwork, self).__init__(\n",
        "#         input_tensor_spec=input_tensor_spec,\n",
        "#         state_spec=(),\n",
        "#         name=name)\n",
        "#     self._sub_layers = [\n",
        "#         tf.keras.layers.Dense(num_actions),\n",
        "#     ]\n",
        "\n",
        "#   def call(self, inputs, step_type=None, network_state=()):\n",
        "#     del step_type\n",
        "#     inputs = tf.cast(inputs, tf.float32)\n",
        "#     for layer in self._sub_layers:\n",
        "#       inputs = layer(inputs)\n",
        "#     return inputs, network_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kv2sBdk2d9jf"
      },
      "outputs": [],
      "source": [
        "hidden_layers = (128, 64)\n",
        "\n",
        "qn = ModifiedQNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    train_env.action_spec(),\n",
        "    fc_layer_params=hidden_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN52ANA9huFf"
      },
      "outputs": [],
      "source": [
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = MaxLegalDqnAgent(genv=train_env,\n",
        "    time_step_spec=train_env.time_step_spec(),\n",
        "    action_spec=train_env.action_spec(),\n",
        "    q_network = qn,\n",
        "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-3),\n",
        "    td_errors_loss_fn = common.element_wise_squared_loss,\n",
        "    train_step_counter = train_step_counter)\n",
        "\n",
        "# def build_agent(model, actions):\n",
        "#     policy = MaxLegalEpsilonGreedyPolicy(q_policy.QPolicy(), UnoEnv())\n",
        "#     memory = SequentialMemory(limit=50000, window_length=1)\n",
        "#     dqn = DqnAgent(model=model, policy=policy, \n",
        "#                   nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
        "#     return dqn\n",
        "\n",
        "# agent = build_agent(model, 60)\n",
        "agent.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCp1h2Xjk4H6"
      },
      "outputs": [],
      "source": [
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sv6mwS7rlOhE"
      },
      "outputs": [],
      "source": [
        "#@test {\"skip\": true}\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jQPYhl5iCQq"
      },
      "outputs": [],
      "source": [
        "num_iterations = 50000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration =   1# @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 3  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "log_interval = 200  # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
        "eval_interval = 1000  # @param {type:\"integer\"}\n",
        "buffer_sequence_length =  2# @param {type:\"integer\"}\n",
        "n_step_update = 2  # @param {type:\"integer\"}\n",
        "replay_buffer_capacity = 100000 # @param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGsPhRqkslRb"
      },
      "outputs": [],
      "source": [
        "# table_name = 'uniform_table'\n",
        "# replay_buffer_signature = tensor_spec.from_spec(\n",
        "#       agent.collect_data_spec)\n",
        "# replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "#     replay_buffer_signature)\n",
        "\n",
        "# table = reverb.Table(\n",
        "#     table_name,\n",
        "#     max_size=replay_buffer_max_length,\n",
        "#     sampler=reverb.selectors.Uniform(),\n",
        "#     remover=reverb.selectors.Fifo(),\n",
        "#     rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "#     signature=replay_buffer_signature)\n",
        "\n",
        "# reverb_server = reverb.Server([table])\n",
        "\n",
        "# replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "#     agent.collect_data_spec,\n",
        "#     table_name=table_name,\n",
        "#     sequence_length=buffer_sequence_length,\n",
        "#     local_server=reverb_server)\n",
        "\n",
        "# rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "#   replay_buffer.py_client,\n",
        "#   table_name,\n",
        "#   sequence_length=buffer_sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is4uRJiN7MTb",
        "outputId": "ecfa9d43-b5cb-4f0d-ffcd-d02a170dee31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
          ]
        }
      ],
      "source": [
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=replay_buffer_capacity)\n",
        "\n",
        "random_policy = LegalRandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec(), train_env)\n",
        "\n",
        "def collect_step(environment, policy):\n",
        "  time_step = environment.current_time_step()\n",
        "  action_step = policy.action(time_step)\n",
        "  next_time_step = environment.step(action_step.action)\n",
        "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "\n",
        "  # Add trajectory to the replay buffer\n",
        "  replay_buffer.add_batch(traj)\n",
        "\n",
        "for _ in range(initial_collect_steps):\n",
        "  collect_step(train_env, random_policy)\n",
        "\n",
        "# This loop is so common in RL, that we provide standard implementations of\n",
        "# these. For more details see the drivers module.\n",
        "\n",
        "# Dataset generates trajectories with shape [BxTx...] where\n",
        "# T = n_step_update + 1.\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3, sample_batch_size=batch_size,\n",
        "    num_steps=n_step_update + 1).prefetch(3)\n",
        "\n",
        "iterator = iter(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1iMsxH4iIZg"
      },
      "outputs": [],
      "source": [
        "# replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "#     agent.collect_data_spec,\n",
        "#     batch_size=batch_size,\n",
        "#     max_length=2000)\n",
        "\n",
        "# metric = py_metrics.AverageReturnMetric()\n",
        "# observers = [metric]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSEvpaeQsz9I",
        "outputId": "e1ee34df-115c-49ce-9ea0-d9e2337a9942"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Trajectory(\n",
              "{'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(60, dtype=int32)),\n",
              " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
              " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
              " 'observation': BoundedTensorSpec(shape=(111,), dtype=tf.float32, name='observation', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
              " 'policy_info': (),\n",
              " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
              " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent.collect_data_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHMUnsnbs0cY",
        "outputId": "52c17d39-e42d-4872-a84d-b8084b6ebcaa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('step_type',\n",
              " 'observation',\n",
              " 'action',\n",
              " 'policy_info',\n",
              " 'next_step_type',\n",
              " 'reward',\n",
              " 'discount')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent.collect_data_spec._fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1ISziAgs1e5"
      },
      "outputs": [],
      "source": [
        "# #@test {\"skip\": true}\n",
        "\n",
        "# py_driver.PyDriver(\n",
        "#     env,\n",
        "#     py_tf_eager_policy.PyTFEagerPolicy(\n",
        "#       random_policy, use_tf_function=True),\n",
        "#     [rb_observer],\n",
        "#     max_steps=initial_collect_steps).run(train_env.reset())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvqQOQMbs3hK",
        "outputId": "190e1e2c-5018-4afa-d341-71b2f79f029f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(Trajectory(\n",
              "{'action': TensorSpec(shape=(3, 2), dtype=tf.int32, name=None),\n",
              " 'discount': TensorSpec(shape=(3, 2), dtype=tf.float32, name=None),\n",
              " 'next_step_type': TensorSpec(shape=(3, 2), dtype=tf.int32, name=None),\n",
              " 'observation': TensorSpec(shape=(3, 2, 111), dtype=tf.float32, name=None),\n",
              " 'policy_info': (),\n",
              " 'reward': TensorSpec(shape=(3, 2), dtype=tf.float32, name=None),\n",
              " 'step_type': TensorSpec(shape=(3, 2), dtype=tf.int32, name=None)}), BufferInfo(ids=TensorSpec(shape=(3, 2), dtype=tf.int64, name=None), probabilities=TensorSpec(shape=(3,), dtype=tf.float32, name=None)))>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Dataset generates trajectories with shape [Bx2x...]\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=buffer_sequence_length).prefetch(3)\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMVI-Ub7s5ml",
        "outputId": "fd52e14d-17d6-43bc-f6ab-5993bd3d6292"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7fbc7b442b50>\n"
          ]
        }
      ],
      "source": [
        "iterator = iter(dataset)\n",
        "print(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL9GUxqStAFO"
      },
      "source": [
        "# **Training DQN Agent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG-csS2KrXYF",
        "outputId": "d9569ae6-ddee-4171-e76c-a1fbddcce3cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
            "Wall time: 9.78 µs\n"
          ]
        }
      ],
      "source": [
        "#@test {\"skip\": true}\n",
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "train_env.reset()\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(train_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFQ031J8EGWR"
      },
      "outputs": [],
      "source": [
        "# # Reset the environment.\n",
        "# time_step = train_env.reset()\n",
        "\n",
        "# # Create a driver to collect experience.\n",
        "# collect_driver = py_driver.PyDriver(\n",
        "#     env,\n",
        "#     py_tf_eager_policy.PyTFEagerPolicy(\n",
        "#       agent.collect_policy, use_tf_function=True),\n",
        "#     [rb_observer],\n",
        "#     max_steps=collect_steps_per_iteration)\n",
        "\n",
        "# for _ in range(num_iterations):\n",
        "\n",
        "#   # Collect a few steps and save to the replay buffer.\n",
        "#   time_step, _ = collect_driver.run(time_step)\n",
        "\n",
        "#   # Sample a batch of data from the buffer and update the agent's network.\n",
        "#   experience, unused_info = next(iterator)\n",
        "#   train_loss = agent.train(experience).loss\n",
        "\n",
        "#   step = agent.train_step_counter.numpy()\n",
        "\n",
        "#   if step % log_interval == 0:\n",
        "#     print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "#   if step % eval_interval == 0:\n",
        "#     avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "#     print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "#     returns.append(avg_return)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koGoam_bakYR"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import os\n",
        "\n",
        "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "\n",
        "\n",
        "tempdir = os.getenv(\"TEST_TMPDIR\", tempfile.gettempdir())\n",
        "checkpoint_dir = os.path.join(tempdir, 'checkpoint')\n",
        "train_checkpointer = common.Checkpointer(\n",
        "    ckpt_dir=checkpoint_dir,\n",
        "    max_to_keep=1,\n",
        "    agent=agent,\n",
        "    policy=agent.policy,\n",
        "    replay_buffer=replay_buffer,\n",
        "    global_step=global_step\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS__7F90cHfT"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "def create_zip_file(dirname, base_filename):\n",
        "  return shutil.make_archive(base_filename, 'zip', dirname)\n",
        "\n",
        "def upload_and_unzip_file_to(dirname):\n",
        "  if files is None:\n",
        "    return\n",
        "  uploaded = files.upload()\n",
        "  for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=fn, length=len(uploaded[fn])))\n",
        "    shutil.rmtree(dirname)\n",
        "    zip_files = zipfile.ZipFile(io.BytesIO(uploaded[fn]), 'r')\n",
        "    zip_files.extractall(dirname)\n",
        "    zip_files.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPN2J2dU8mzp",
        "outputId": "45849a04-5fe7-4174-b413-48521709ca34"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py:265: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step = 99800: loss = 0.0016580215888097882\n",
            "step = 100000: loss = 9.555965516483411e-05\n",
            "step = 100000: Average Return = -0.10070312023162842\n",
            "step = 100200: loss = 2.877583256122307e-06\n",
            "step = 100400: loss = 5.175410115043633e-05\n",
            "step = 100600: loss = 1.4860628425594768e-06\n",
            "step = 100800: loss = 0.0004381748440209776\n",
            "step = 101000: loss = 0.0009738858789205551\n",
            "step = 101000: Average Return = -0.031953126192092896\n",
            "step = 101200: loss = 0.0013999887742102146\n",
            "step = 101400: loss = 0.0009918485302478075\n",
            "step = 101600: loss = 0.0005441975663416088\n",
            "step = 101800: loss = 0.00025428240769542754\n",
            "step = 102000: loss = 0.0030887520406395197\n",
            "step = 102000: Average Return = -0.08015625178813934\n",
            "step = 102200: loss = 0.00011995690874755383\n",
            "step = 102400: loss = 0.000328419468132779\n",
            "step = 102600: loss = 0.0003875835391227156\n",
            "step = 102800: loss = 0.005870296154171228\n",
            "step = 103000: loss = 0.0010258810361847281\n",
            "step = 103000: Average Return = -0.0658593699336052\n",
            "step = 103200: loss = 4.6287528675748035e-05\n",
            "step = 103400: loss = 0.00012590417463798076\n",
            "step = 103600: loss = 0.00025224697310477495\n",
            "step = 103800: loss = 0.0003633451124187559\n",
            "step = 104000: loss = 5.853732091054553e-06\n",
            "step = 104000: Average Return = -0.036796875298023224\n",
            "step = 104200: loss = 0.00021494629618246108\n",
            "step = 104400: loss = 0.00018137875304091722\n",
            "step = 104600: loss = 0.0002961624995805323\n",
            "step = 104800: loss = 0.00011176529369549826\n",
            "step = 105000: loss = 0.0002648511144798249\n",
            "step = 105000: Average Return = -0.015625\n",
            "step = 105200: loss = 0.00013246870366856456\n",
            "step = 105400: loss = 0.0005589837674051523\n",
            "step = 105600: loss = 0.00022579253709409386\n",
            "step = 105800: loss = 1.776888530002907e-05\n",
            "step = 106000: loss = 0.00010940620995825157\n",
            "step = 106000: Average Return = -0.030312499031424522\n",
            "step = 106200: loss = 0.0005358056514523923\n",
            "step = 106400: loss = 0.0003141954366583377\n",
            "step = 106600: loss = 0.005044532008469105\n",
            "step = 106800: loss = 0.00023183618031907827\n",
            "step = 107000: loss = 0.000260258762864396\n",
            "step = 107000: Average Return = -0.049687501043081284\n",
            "step = 107200: loss = 0.0022428419906646013\n",
            "step = 107400: loss = 4.400685429573059e-05\n",
            "step = 107600: loss = 0.00013887106615584344\n",
            "step = 107800: loss = 4.210538827464916e-05\n",
            "step = 108000: loss = 0.000540899985935539\n",
            "step = 108000: Average Return = -0.04257812350988388\n",
            "step = 108200: loss = 0.004691428039222956\n",
            "step = 108400: loss = 2.04344487428898e-05\n",
            "step = 108600: loss = 0.00015348232409451157\n",
            "step = 108800: loss = 0.0048934221267700195\n",
            "step = 109000: loss = 3.1230247259372845e-05\n",
            "step = 109000: Average Return = -0.07117187976837158\n",
            "step = 109200: loss = 0.0001773651601979509\n",
            "step = 109400: loss = 0.0001371611433569342\n",
            "step = 109600: loss = 0.00045950047206133604\n",
            "step = 109800: loss = 3.16147597914096e-05\n",
            "step = 110000: loss = 0.0006124151987023652\n",
            "step = 110000: Average Return = -0.013828125782310963\n",
            "step = 110200: loss = 0.003000908298417926\n",
            "step = 110400: loss = 0.00017239195585716516\n",
            "step = 110600: loss = 6.0447666328400373e-05\n",
            "step = 110800: loss = 0.0034828661009669304\n",
            "step = 111000: loss = 2.538792796258349e-05\n",
            "step = 111000: Average Return = -0.03242187574505806\n",
            "step = 111200: loss = 0.0002812734164763242\n",
            "step = 111400: loss = 0.0004698148986790329\n",
            "step = 111600: loss = 0.00012179711484350264\n",
            "step = 111800: loss = 0.00010492103319847956\n",
            "step = 112000: loss = 0.0005022602854296565\n",
            "step = 112000: Average Return = -0.07117187976837158\n",
            "step = 112200: loss = 0.00013715249951928854\n",
            "step = 112400: loss = 0.0001046156685333699\n",
            "step = 112600: loss = 0.0005370224826037884\n",
            "step = 112800: loss = 0.0004112405877094716\n",
            "step = 113000: loss = 9.687676356406882e-05\n",
            "step = 113000: Average Return = -0.037578124552965164\n",
            "step = 113200: loss = 0.0005065481527708471\n",
            "step = 113400: loss = 0.0019433576380833983\n",
            "step = 113600: loss = 5.494049037224613e-05\n",
            "step = 113800: loss = 4.006164090242237e-05\n",
            "step = 114000: loss = 5.757562757935375e-05\n",
            "step = 114000: Average Return = -0.0514843687415123\n",
            "step = 114200: loss = 0.0007318794378079474\n",
            "step = 114400: loss = 6.210550054674968e-05\n",
            "step = 114600: loss = 0.0005408629076555371\n",
            "step = 114800: loss = 0.0002578790008556098\n",
            "step = 115000: loss = 0.00024578857119195163\n",
            "step = 115000: Average Return = -0.054843753576278687\n",
            "step = 115200: loss = 7.6658783655148e-05\n",
            "step = 115400: loss = 0.0026677825953811407\n"
          ]
        }
      ],
      "source": [
        "train_env.reset()\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "  for _ in range(collect_steps_per_iteration):\n",
        "    collect_step(train_env, agent.collect_policy)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  # print(\"DEBUG - EXPERIENCE:\", experience)\n",
        "  train_loss = agent.train(experience)\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
        "\n",
        "  # if step % 5000 == 0:\n",
        "  #   train_checkpointer.save(global_step)\n",
        "  #   checkpoint_zip_filename = create_zip_file(checkpoint_dir, os.path.join(tempdir, 'exported_cp'))\n",
        "\n",
        "  #   if files is not None:\n",
        "  #     files.download(checkpoint_zip_filename) \n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(train_env, agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "583xsTlWWvoe"
      },
      "outputs": [],
      "source": [
        "returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtyv-cZWB77q"
      },
      "outputs": [],
      "source": [
        "train_checkpointer.save(global_step)\n",
        "checkpoint_zip_filename = create_zip_file(checkpoint_dir, os.path.join(tempdir, 'exported_cp'))\n",
        "\n",
        "if files is not None:\n",
        "      files.download(checkpoint_zip_filename) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgRkCmTYBfL9"
      },
      "outputs": [],
      "source": [
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(iterations, returns[:51])\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylim(top=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEoLuT7VV1Oz"
      },
      "outputs": [],
      "source": [
        "%debug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8-rqy1Hs6X1"
      },
      "outputs": [],
      "source": [
        "class DQN_UnoPlayer(UnoPlayer):\n",
        "    def play(self, hand, up_card, called_color, state):\n",
        "        for i in range(len(hand)):\n",
        "            if self.can_play_on(hand[i], up_card, called_color):\n",
        "                return i\n",
        "        return -1\n",
        "\n",
        "    def call_color(self, hand):\n",
        "        return self.Color.RED\n",
        "\n",
        "    def can_play_on(self, card, up_card, called_color):\n",
        "        result = card.get_rank() == self.Rank.WILD\n",
        "        result = result or card.get_rank() == self.Rank.WILD_D4\n",
        "        result = result or card.get_color() == up_card.get_color()\n",
        "        result = result or card.get_color() == called_color\n",
        "        result = result or ((card.get_rank() == up_card.get_rank()) and (card.get_rank() != self.Rank.NUMBER))\n",
        "        result = result or (card.get_number() == up_card.get_number() and card.get_rank() == self.Rank.NUMBER and up_card.get_rank() == self.Rank.NUMBER)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Shb_1efuIYkc"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "uno_dqn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}